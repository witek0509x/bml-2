#!/bin/bash
#SBATCH --job-name=multi_node
#SBATCH --output=log_multi_node_%j.out
#SBATCH --error=log_multi_node%j.err
#SBATCH --partition=plgrid-gpu-a100
#SBATCH --account=plgllmparamgr-gpu-a100
#SBATCH --nodes=2
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-gpu=4
#SBATCH --time=0-00:30:00
#SBATCH --mem=128G
#SBATCH --array=1-3

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

config=./grid_search.conf
sample=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config)

echo "Starting task with lr = " $sample


echo Node IP: $head_node_ip
export LOGLEVEL=INFO

source $HOME/venv/bin/activate
source $SCRATCH/bml-2/.env

srun torchrun \
  --nnodes 2 \
  --nproc_per_node 1 \
  --rdzv_id $RANDOM \
  --rdzv_backend c10d \
  --rdzv_endpoint $head_node_ip \
  ./main.py \
  --n_training_steps 8500 \
  --batch_size 256 \
  --n_layers 4 \
  --dmodel 256 \
  --n_heads 4 \
  --seq_len 256 \
  --dropout 0.0 \
  --lr $sample